{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "\n",
    "def square_loss(y_true, y_pred):\n",
    "    loss = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def absolute_loss(y_true, y_pred):\n",
    "    loss = tf.abs(y_true - y_pred)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    \"\"\"Compute L1-smooth loss.\n",
    "    # Arguments\n",
    "        y_true: Ground truth bounding boxes,\n",
    "            tensor of shape (?, num_boxes, 4).\n",
    "        y_pred: Predicted bounding boxes,\n",
    "            tensor of shape (?, num_boxes, 4).\n",
    "    # Returns\n",
    "        l1_loss: L1-smooth loss, tensor of shape (?, num_boxes).\n",
    "    # References\n",
    "        https://arxiv.org/abs/1504.08083\n",
    "    \"\"\"\n",
    "    abs_loss = tf.abs(y_true - y_pred)\n",
    "    sq_loss = 0.5 * (y_true - y_pred)**2\n",
    "    loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def softmax_loss(y_true, y_pred):\n",
    "    \"\"\"Compute cross entropy loss aka softmax loss.\n",
    "    # Arguments\n",
    "        y_true: Ground truth targets,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "        y_pred: Predicted logits,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "    # Returns\n",
    "        softmax_loss: Softmax loss, tensor of shape (?, num_boxes).\n",
    "    \"\"\"\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    loss = - y_true * tf.log(y_pred)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"Compute binary cross entropy loss.\n",
    "    \"\"\"\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    loss = - y_true*tf.log(y_pred) - (1.-y_true)*tf.log(1.-y_pred)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def focal_loss(y_true, y_pred, gamma=2., alpha=1.):\n",
    "    \"\"\"Compute binary focal loss.\n",
    "    \n",
    "    # Arguments\n",
    "        y_true: Ground truth targets,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "        y_pred: Predicted logits,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "    \n",
    "    # Returns\n",
    "        focal_loss: Focal loss, tensor of shape (?, num_boxes).\n",
    "    # References\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "    \"\"\"\n",
    "    #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    #loss = - K.pow(1-y_pred, gamma) * y_true*tf.log(y_pred) - K.pow(y_pred, gamma) * (1-y_true)*tf.log(1-y_pred)\n",
    "    pt = tf.where(tf.equal(y_true, 1.), y_pred, 1.-y_pred)\n",
    "    loss = - K.pow(1.-pt, gamma) * K.log(pt)\n",
    "    loss = alpha * loss\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "def reduced_focal_loss(y_true, y_pred, gamma=2., alpha=1., th=0.5):\n",
    "    \"\"\"Compute binary reduced focal loss.\n",
    "    \n",
    "    # Arguments\n",
    "        y_true: Ground truth targets,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "        y_pred: Predicted logits,\n",
    "            tensor of shape (?, num_boxes, num_classes).\n",
    "    \n",
    "    # Returns\n",
    "        reduced_focal_loss: Reduced focal loss, tensor of shape (?, num_boxes).\n",
    "    # References\n",
    "        https://arxiv.org/abs/1903.01347\n",
    "    \"\"\"\n",
    "    #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    pt = tf.where(tf.equal(y_true, 1.), y_pred, 1.-y_pred)\n",
    "    fr = tf.where(tf.less(pt, th), K.ones_like(pt), K.pow(1.-pt, gamma)/(th**gamma))\n",
    "    loss = - fr * K.log(pt)\n",
    "    loss = alpha * loss\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class LearningRateDecay(Callback):\n",
    "    def __init__(self, methode='linear', base_lr=1e-3, n_desired=40000, desired=0.1, bias=0.0, minimum=0.1):\n",
    "        super(LearningRateDecay, self).__init__()\n",
    "        self.methode = methode\n",
    "        self.base_lr = base_lr\n",
    "        self.n_desired = n_desired\n",
    "        self.desired = desired\n",
    "        self.bias = bias\n",
    "        self.minimum = minimum\n",
    "        \n",
    "        #TODO: better naming\n",
    "\n",
    "    def compute_learning_rate(self, n, methode):\n",
    "        n_desired = self.n_desired\n",
    "        desired = self.desired\n",
    "        base_lr = self.base_lr\n",
    "        bias = self.bias\n",
    "        \n",
    "        offset = base_lr * desired * bias\n",
    "        base_lr = base_lr - offset\n",
    "        \n",
    "        desired = desired / (1-desired*bias) * (1-bias)\n",
    "        \n",
    "        if methode == 'default':\n",
    "            k = (1 - desired) / n_desired\n",
    "            lr = np.maximum( -k * n + 1, 0)\n",
    "        elif methode == 'linear':\n",
    "            k = (1 / desired - 1) / n_desired\n",
    "            lr = 1 / (1 + k * n)\n",
    "        elif methode == 'quadratic':\n",
    "            k = (np.sqrt(1/desired)-1) / n_desired\n",
    "            lr = 1 / (1 + k * n)**2\n",
    "        elif methode == 'exponential':\n",
    "            k = -1 * np.log(desired) / n_desired\n",
    "            lr = np.exp(-k*n)\n",
    "        \n",
    "        lr = base_lr * lr + offset\n",
    "        lr = np.maximum(lr, self.base_lr * self.minimum)\n",
    "        return lr\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch = batch\n",
    "        steps_per_epoch = self.params['steps']\n",
    "        iteration = self.epoch * steps_per_epoch + batch\n",
    "        \n",
    "        lr = self.compute_learning_rate(iteration, self.methode)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def plot_learning_rates(self):\n",
    "        n = np.linspace(0, self.n_desired*2, 101)\n",
    "        plt.figure(figsize=[16, 6])\n",
    "        plt.plot([n[0], n[-1]], [self.base_lr*self.desired*self.bias]*2, 'k')\n",
    "        for m in ['default', 'linear', 'quadratic', 'exponential']:\n",
    "            plt.plot(n, self.compute_learning_rate(n, m))\n",
    "        plt.legend(['bias', '$-kn+1$', '$1/(1+kn)$', '$1/(1+kn)^2$', '$e^{-kn}$'])\n",
    "        plt.grid()\n",
    "        plt.xlim(0, n[-1])\n",
    "        plt.ylim(0, None)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class ModelSnapshot(Callback):\n",
    "    \"\"\"Save the model weights after an interval of iterations.\"\"\"\n",
    "    \n",
    "    def __init__(self, logdir, interval=10000, verbose=1):\n",
    "        super(ModelSnapshot, self).__init__()\n",
    "        self.logdir = logdir\n",
    "        self.interval = interval\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch = batch\n",
    "        # steps/batches/iterations\n",
    "        steps_per_epoch = self.params['steps']\n",
    "        self.iteration = self.epoch * steps_per_epoch + batch + 1\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.iteration % self.interval == 0:\n",
    "            filepath = os.path.join(self.logdir, 'weights.%06i.h5' % (self.iteration))\n",
    "            if self.verbose > 0:\n",
    "                print('\\nSaving model %s' % (filepath))\n",
    "            self.model.save_weights(filepath, overwrite=True)\n",
    "\n",
    "\n",
    "class Logger(Callback):\n",
    "    \n",
    "    def __init__(self, logdir):\n",
    "        super(Logger, self).__init__()\n",
    "        self.logdir = logdir\n",
    "    \n",
    "    def save_history(self):\n",
    "        df = pd.DataFrame.from_dict(self.model.history.history)\n",
    "        df.to_csv(os.path.join(self.logdir, 'history.csv'), index=False)\n",
    "    \n",
    "    def append_log(self, logs):\n",
    "        data = {k:[float(logs[k])] for k in self.model.metrics_names}\n",
    "        data['iteration'] = [self.iteration]\n",
    "        data['epoch'] = [self.epoch]\n",
    "        data['batch'] = [self.batch]\n",
    "        data['time'] = [time.time() - self.start_time]\n",
    "        #data['lr'] = [float(K.get_value(self.model.optimizer.lr))]\n",
    "        df = pd.DataFrame.from_dict(data)\n",
    "        with open(os.path.join(self.logdir, 'log.csv'), 'a') as f:\n",
    "            df.to_csv(f, header=f.tell()==0, index=False)\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        self.save_history()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch = batch\n",
    "        # steps/batches/iterations\n",
    "        steps_per_epoch = self.params['steps']\n",
    "        self.iteration = self.epoch * steps_per_epoch + batch\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.append_log(logs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.save_history()\n",
    "\n",
    "\n",
    "def filter_signal(x, y, window_length=1000):\n",
    "    if type(window_length) is not int or len(y) <= window_length:\n",
    "        return [], []\n",
    "    \n",
    "    #w = np.ones(window_length) # moving average\n",
    "    w = np.hanning(window_length) # hanning window\n",
    "    \n",
    "    wlh = int(window_length/2)\n",
    "    if x is None:\n",
    "        x = np.arange(wlh, len(y)-wlh+1)\n",
    "    else:\n",
    "        x = x[wlh:len(y)-wlh+1]\n",
    "    y = np.convolve(w/w.sum(), y, mode='valid')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_log(log_dir, names=None, limits=None, window_length=250, log_dir_compare=None):\n",
    "    \n",
    "    # TODO: differnet batch sizes lead to different epoch length\n",
    "    \n",
    "    if limits is None:\n",
    "        limits = slice(None)\n",
    "    elif type(limits) in [list, tuple]:\n",
    "        limits = slice(*limits)\n",
    "    \n",
    "    print(log_dir)\n",
    "    d = pd.read_csv(os.path.join('.', 'checkpoints', log_dir, 'log.csv'))\n",
    "    d = d[limits]\n",
    "    iteration = np.array(d['iteration'])\n",
    "    epoch = np.array(d['epoch'])\n",
    "    idx = np.argwhere(np.diff(epoch))[:,0] + 1\n",
    "    \n",
    "    if log_dir_compare is not None:\n",
    "        print(log_dir_compare)\n",
    "        d2 = pd.read_csv(os.path.join('.', 'checkpoints', log_dir_compare, 'log.csv'))\n",
    "        d2 = d2[limits]\n",
    "        iteration2 = np.array(d2['iteration'])\n",
    "        \n",
    "    if names is None:\n",
    "        names = set(d.keys())\n",
    "    else:\n",
    "        names = set(names)\n",
    "        names.intersection_update(set(d.keys()))\n",
    "    if log_dir_compare is not None:\n",
    "        names.intersection_update(set(d2.keys()))\n",
    "    names.difference_update({'epoch', 'batch', 'iteration', 'time'})\n",
    "    print(names)\n",
    "    \n",
    "    if 'time' in d.keys() and len(idx) > 1:\n",
    "        t = np.array(d['time'])\n",
    "        print('time per epoch %3.1f h' % ((t[idx[1]]-t[idx[0]])/3600))\n",
    "    \n",
    "    # reduce epoch ticks\n",
    "    max_ticks = 20\n",
    "    n = len(idx)\n",
    "    if n > 1:\n",
    "        n = round(n,-1*int(np.floor(np.log10(n))))\n",
    "        while n >= max_ticks:\n",
    "            if n/2 < max_ticks:\n",
    "                n /= 2\n",
    "            else:\n",
    "                if n/5 < max_ticks:\n",
    "                    n /= 5\n",
    "                else:\n",
    "                    n /= 10\n",
    "        idx_step = int(np.ceil(len(idx)/n))\n",
    "        epoch_step = epoch[idx[idx_step]] - epoch[idx[0]]\n",
    "        for first_idx in range(len(idx)):\n",
    "            if epoch[idx[first_idx]] % epoch_step == 0:\n",
    "                break\n",
    "        idx_red = [idx[i] for i in range(first_idx, len(idx), idx_step)]\n",
    "    else:\n",
    "        idx_red = idx\n",
    "    \n",
    "    for k in names:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.plot(iteration, d[k], zorder=0)\n",
    "        plt.plot(*filter_signal(iteration, d[k], window_length))\n",
    "        plt.title(k, y=1.05)\n",
    "        \n",
    "        # second log\n",
    "        if log_dir_compare is not None:\n",
    "            plt.plot(iteration2, d2[k], zorder=0)\n",
    "            plt.plot(*filter_signal(iteration2, d2[k], window_length))\n",
    "            xmin = min(iteration[0], iteration2[0])\n",
    "            xmax = max(iteration[-1], iteration2[-1])\n",
    "        else:\n",
    "            xmin = iteration[0]\n",
    "            xmax = iteration[-1]\n",
    "        \n",
    "        ax1 = plt.gca()\n",
    "        ax1.set_xlim(xmin, xmax)\n",
    "        ax1.yaxis.grid(True)\n",
    "        #ax1.set_xlabel('iteration')\n",
    "        #ax1.set_yscale('linear')\n",
    "        ax1.get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "        \n",
    "        ax2 = ax1.twiny()\n",
    "        ax2.xaxis.grid(True)\n",
    "        ax2.set_xticks(iteration[idx_red])\n",
    "        ax2.set_xticklabels(epoch[idx_red])\n",
    "        ax2.set_xlim(xmin, xmax)\n",
    "        #ax2.set_xlabel('epoch')\n",
    "        #ax2.set_yscale('linear')\n",
    "        ax2.get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "        \n",
    "        k_end = k.split('_')[-1]\n",
    "        if k_end in ['loss']:\n",
    "            ymin = 0\n",
    "            ymax = min(np.max(d[k][np.isfinite(d[k])]), np.mean(d[k][np.isfinite(d[k])])*8)\n",
    "            ax1.set_ylim(ymin, ymax)\n",
    "        if k_end in ['precision', 'recall', 'fmeasure', 'accuracy']:\n",
    "            ax1.set_ylim(0, 1)\n",
    "        \n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
